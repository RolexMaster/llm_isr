from __future__ import annotations

# =========================
# íŒ¨í‚¤ì§€ ë²„ì „ / ì»¤ë°‹ ì •ë³´ (pip ì„¤ì¹˜ ë²„ì „ í™•ì¸ìš©)
# =========================
# ë¦´ë¦¬ìŠ¤í•  ë•Œë§ˆë‹¤ __version__ ê°’ì„ pyproject.toml / setup.cfg ì˜ ë²„ì „ê³¼
# ë°˜ë“œì‹œ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ì„¸ìš”.
__version__ = "0.1.0"
__commit__ = "dev"  # í•„ìš”í•˜ë©´ ì‹¤ì œ git short SHA ë“±ìœ¼ë¡œ êµì²´í•´ì„œ ì‚¬ìš©

import json
import time
import re
import logging
from typing import Dict, Any, List, Tuple

from openai import OpenAI
from fastmcp import Client

from agent_prompts import build_system_prompt, AgentLanguage  # ğŸ”¹ ìƒˆë¡œ ì¶”ê°€


# =========================
# ë¡œì»¬ ìœ í‹¸
# =========================
def pretty_json(obj: Any, limit: int = 1000) -> str:
    try:
        s = json.dumps(obj, ensure_ascii=False, indent=2)
    except Exception:
        s = str(obj)
    return s if len(s) <= limit else s[:limit] + "\n... (truncated)"


# (í˜„ì¬ëŠ” ì •ê·œì‹ ë¸”ë¡ì€ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, í•„ìš”ì‹œë¥¼ ìœ„í•´ ë‚¨ê²¨ë‘ )
TOOLCALL_BLOCK = re.compile(
    r"<tool_call>\s*(\{.*?\})\s*(?:</tool_call>|$)", re.DOTALL
)


def extract_toolcalls_from_text(text: str) -> List[Dict[str, Any]]:
    """
    ëª¨ë¸ì´ tool_callsë¥¼ êµ¬ì¡°í™”í•´ ì£¼ì§€ ì•Šê³ ,
    í…ìŠ¤íŠ¸ë¡œ <tool_call>{ "name": ..., "arguments": {...} }</tool_call> ë¥¼ ë‚¸ ê²½ìš° íŒŒì‹±.
    ì •ê·œì‹ ëŒ€ì‹  ì¤‘ê´„í˜¸ ë°¸ëŸ°ì‹±ìœ¼ë¡œ ì²« ë²ˆì§¸ JSON ë¸”ë¡ì„ ìµœëŒ€í•œ ê´€ëŒ€í•˜ê²Œ ì¶”ì¶œí•œë‹¤.
    - <tool_call>{...}</tool_call> í˜•íƒœëŠ” ë¬¼ë¡ 
    - <tool_call>{...}<tool_call> ì²˜ëŸ¼ ë‹«ëŠ” íƒœê·¸ê°€ ë¹ ì ¸ ìˆì–´ë„
      ì²« ë²ˆì§¸ {...} ë¸”ë¡ë§Œ ì˜ë¼ì„œ íŒŒì‹±ì„ ì‹œë„í•œë‹¤.
    """
    out: List[Dict[str, Any]] = []
    if not text:
        return out

    # '<tool_call>' ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ì„œ ê° ë¸”ë¡ì—ì„œ JSON ë¶€ë¶„ì„ ì°¾ëŠ”ë‹¤.
    parts = text.split("<tool_call>")
    if len(parts) <= 1:
        return out

    for part in parts[1:]:
        # part ì˜ˆ:
        #   '{"name": "...", "arguments": {...}}</tool_call> bla bla'
        #   '{"name": "...", "arguments": {...}}<tool_call>{...}</tool_call>...'
        # ì—ì„œ ì²« ë²ˆì§¸ {...}ë§Œ ì¶”ì¶œ
        brace_start = part.find("{")
        if brace_start < 0:
            continue

        depth = 0
        brace_end = None
        for idx in range(brace_start, len(part)):
            ch = part[idx]
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    brace_end = idx
                    break

        if brace_end is None:
            # ì¤‘ê´„í˜¸ ê· í˜•ì´ ë§ì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ
            continue

        json_str = part[brace_start: brace_end + 1]

        try:
            obj = json.loads(json_str)
        except Exception:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
            continue

        name = obj.get("name")
        args = obj.get("arguments", {}) or {}
        if isinstance(name, str) and isinstance(args, dict):
            out.append({"name": name, "arguments": args})

    return out


def requires_params_wrapper(schema_json: dict) -> bool:
    """ì„œë²„ê°€ ìµœìƒìœ„ {"params": {...}} í˜•íƒœë¥¼ ìš”êµ¬í•˜ëŠ”ì§€ ì…ë ¥ ìŠ¤í‚¤ë§ˆë¡œ ì¶”ì •."""
    if not isinstance(schema_json, dict):
        return False
    props = schema_json.get("properties", {})
    req = set(schema_json.get("required", []))
    return ("params" in props) and ("params" in req)


def build_name_maps(mcp_tools: List[Any]) -> Tuple[Dict[str, str], Dict[str, str]]:
    """llm_func_name <-> mcp_name ë§¤í•‘ ìƒì„±"""
    f2m: Dict[str, str] = {}
    m2f: Dict[str, str] = {}
    for t in mcp_tools:
        name = getattr(t, "name", None) or t.get("name")
        if not name:
            continue
        llm_name = name.replace(".", "_")
        f2m[llm_name] = name
        m2f[name] = llm_name
    return f2m, m2f


def build_tools_block(mcp_tools: List[Any]) -> str:
    """
    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•  ê°„ë‹¨í•œ ë„êµ¬ ëª©ë¡(JSON).
    nameê³¼ parametersë§Œ ì‹£ëŠ”ë‹¤.
    """
    items: List[Dict[str, Any]] = []
    for t in mcp_tools:
        name = getattr(t, "name", None) or t.get("name")
        if not name:
            continue

        schema = (
            getattr(t, "input_schema", None)
            or getattr(t, "inputSchema", None)
            or t.get("input_schema")
            or t.get("inputSchema")
            or {"type": "object", "properties": {}}
        )
        if hasattr(schema, "model_dump"):
            schema = schema.model_dump()
        if not isinstance(schema, dict):
            schema = {"raw": str(schema)}

        items.append(
            {
                "name": name.replace(".", "_"),
                "parameters": schema,
            }
        )
    return json.dumps(items, ensure_ascii=False, indent=2)


# =========================
# BridgeSession
# =========================
class BridgeSession:
    """
    - LLM, MCP í´ë¼ì´ì–¸íŠ¸, MCP ë„êµ¬ëª©ë¡ì„ ê°€ì§„ ëŒ€í™” ì„¸ì…˜ ì»¨í…Œì´ë„ˆ
    - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±/íˆìŠ¤í† ë¦¬ ìœ ì§€
    - ì‚¬ìš©ì ì…ë ¥ 1ê±´ì„ ì²˜ë¦¬(handle_one_turn): í•„ìš” ì‹œ MCP ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê³ , ìµœì¢… LLM ì¶œë ¥ ë¬¸ìì—´ì„ ë°˜í™˜
    - debug=True ì¼ ë•ŒëŠ” LLM ê° í„´/íˆ´ í˜¸ì¶œê¹Œì§€ ëª¨ë‘ ë‹´ì€ dict ë°˜í™˜
    """

    # íŒ¨í‚¤ì§€ ë²„ì „/ì»¤ë°‹ ì •ë³´ ë…¸ì¶œ (ë””ë²„ê¹…/ë²„ì „ í™•ì¸ìš©)
    VERSION: str = __version__
    COMMIT: str = __commit__

    def __init__(
        self,
        llm: OpenAI,
        mcp: Client,
        mcp_tools: List[Any],
        *,
        temp: float = 0.2,
        max_tokens: int = 512,
        max_turns: int = 3,
        language: AgentLanguage = "en",  # ğŸ”¹ Llama ê¸°ë³¸ ì˜ì–´ ì—ì´ì „íŠ¸
    ):
        self.llm = llm
        self.mcp = mcp
        self.mcp_tools = mcp_tools
        self.temp = temp
        self.max_tokens = max_tokens
        self.max_turns = max_turns
        self.language = language
        self.logger = logging.getLogger("mcp_bridge")

        # LLM í•¨ìˆ˜ëª… <-> MCP ë„êµ¬ëª… ë§¤í•‘
        self.f2m, _ = build_name_maps(mcp_tools)
        self.tools_block = build_tools_block(mcp_tools)

        # ğŸ”¹ ì–¸ì–´ì— ë§ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        self.sys_prompt = build_system_prompt(
            language=self.language,
            tools_block=self.tools_block,
        )

        self.messages: List[Dict[str, Any]] = [
            {"role": "system", "content": self.sys_prompt}
        ]

    def reset(self) -> None:
        self.messages = [{"role": "system", "content": self.sys_prompt}]
        self.logger.info("[SESSION] messages reset")

    async def handle_one_turn(
        self,
        user_text: str,
        *,
        model: str,
        debug: bool = False,
    ) -> Any:
        """
        - ì‚¬ìš©ì ì…ë ¥ 1ê±´ ì²˜ë¦¬(í•„ìš”í•˜ë©´ MCP ë„êµ¬ í˜¸ì¶œ í¬í•¨)
        - debug=False (ê¸°ë³¸ê°’): ìµœì¢… LLM ì¶œë ¥ ë¬¸ìì—´ë§Œ ë°˜í™˜
        - debug=True: LLM ê° í„´ì˜ raw ì¶œë ¥, tool_calls, tool_resultsê¹Œì§€ ëª¨ë‘ ë‹´ì€ dict ë°˜í™˜
        """
        self.messages.append({"role": "user", "content": user_text})
        self.logger.debug(
            "[LLM INPUT FULL]\n" + pretty_json(self.messages, limit=100000)
        )

        assistant_text: str = ""
        turns_debug: List[Dict[str, Any]] = []

        # ë„êµ¬ í˜¸ì¶œì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœëŒ€ self.max_turns ë°˜ë³µ
        for turn in range(1, self.max_turns + 1):
            t0 = time.perf_counter()
            resp = self.llm.chat.completions.create(
                model=model,
                messages=self.messages,
                tool_choice="none",  # vLLM ë‚´ì¥ íŒŒì„œ ë¹„í™œì„±í™” (í´ë°± íŒŒì„œ ì‚¬ìš©)
                temperature=self.temp,
                max_tokens=self.max_tokens,
            )
            dt = time.perf_counter() - t0

            msg = resp.choices[0].message
            self.logger.info(
                f"[LLM] response id: {getattr(resp, 'id', '<no-id>')} ({dt:.2f}s), "
                f"finish_reason={resp.choices[0].finish_reason}"
            )
            self.logger.debug(f"[LLM] assistant content:\n{msg.content}")

            assistant_text = (msg.content or "").strip()

            # í´ë°± íŒŒì„œë¡œ tool_call ì¶”ì¶œ
            calls: List[Dict[str, Any]] = []
            if msg.content and "<tool_call>" in msg.content:
                calls = extract_toolcalls_from_text(msg.content)
                self.logger.info(
                    f"[LLM] fallback extracted tool_calls: {calls}"
                )

            # ì´ë²ˆ í„´ì— ëŒ€í•œ ë””ë²„ê·¸ ì •ë³´ í‹€
            turn_info: Dict[str, Any] = {
                "turn_index": turn,
                "llm_raw": assistant_text,  # LLMì´ ê·¸ëŒ€ë¡œ ë‚¸ í…ìŠ¤íŠ¸
                "tool_calls": calls,        # íŒŒì‹±ëœ ë„êµ¬ í˜¸ì¶œë“¤
                "tool_results": [],         # ì•„ë˜ì—ì„œ ì±„ì›€
            }

            # ë„êµ¬ í˜¸ì¶œì´ ì—†ë‹¤ë©´ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì¢…ë£Œ
            if not calls:
                # ì°¸ê³ : ì—¬ê¸°ì„œ <tool_response>ê°€ ì„ì—¬ ìˆìœ¼ë©´ ê²½ê³ ë§Œ ë‚¨ê¸°ê³  ê·¸ëŒ€ë¡œ ë°˜í™˜
                if "<tool_response>" in assistant_text:
                    self.logger.warning(
                        "[BRIDGE] LLM emitted forbidden <tool_response> tag in a non-tool_call turn."
                    )
                turns_debug.append(turn_info)
                if debug:
                    return {
                        "final_answer": assistant_text,
                        "turns": turns_debug,
                    }
                return assistant_text

            # ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰
            all_ok = True
            tool_results_turn: List[Dict[str, Any]] = []

            for i, c in enumerate(calls, 1):
                func_name = c["name"]
                args = c.get("arguments", {})
                self.logger.info(
                    f"[TOOLCALL {i}] func={func_name} args={json.dumps(args, ensure_ascii=False)}"
                )
                mcp_name = self.f2m.get(func_name)
                self.logger.info(
                    f"[TOOLCALL {i}] mapped LLM '{func_name}' -> MCP '{mcp_name}'"
                )

                # ê¸°ë³¸ payloadëŠ” ê·¸ëƒ¥ args (mcp_nameì´ ì—†ì„ ë•Œë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
                payload: Dict[str, Any] = args
                tool_result: Dict[str, Any]

                if not mcp_name:
                    tool_result = {
                        "ok": False,
                        "error": f"unknown_tool_mapping:{func_name}",
                    }
                    all_ok = False
                    self.logger.error(
                        f"[TOOLCALL {i}] ERROR {tool_result['error']}"
                    )
                else:
                    # í•„ìš”ì‹œ {"params": {...}} ë˜í•‘
                    schema_obj = next(
                        (
                            getattr(t, "input_schema", None)
                            or getattr(t, "inputSchema", None)
                            or t.get("input_schema")
                            or t.get("inputSchema")
                            for t in self.mcp_tools
                            if (getattr(t, "name", None) or t.get("name")) == mcp_name
                        ),
                        None,
                    )

                    if hasattr(schema_obj, "model_dump"):
                        schema_json = schema_obj.model_dump()
                    elif isinstance(schema_obj, dict):
                        schema_json = schema_obj
                    else:
                        schema_json = {}

                    payload = (
                        {"params": args}
                        if requires_params_wrapper(schema_json)
                        else args
                    )

                    try:
                        self.logger.info(
                            f"[TOOLCALL {i}] calling MCP tools/call name={mcp_name} payload={payload}"
                        )
                        t1 = time.perf_counter()
                        res = await self.mcp.call_tool(mcp_name, payload)
                        dtt = time.perf_counter() - t1
                        tool_result = getattr(res, "data", res)
                        self.logger.info(
                            f"[TOOLCALL {i}] MCP result ({dtt:.2f}s):\n{pretty_json(tool_result)}"
                        )
                        # tool_result í˜•ì‹ ë°©ì–´: dict ì´ë©´ì„œ ok=True ì¼ ë•Œë§Œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                        if not (isinstance(tool_result, dict) and tool_result.get("ok", False)):
                            all_ok = False
                    except Exception as e:
                        tool_result = {"ok": False, "error": str(e)}
                        all_ok = False
                        self.logger.exception(
                            f"[TOOLCALL {i}] MCP ERROR: {e}"
                        )

                # ì´ë²ˆ tool í˜¸ì¶œì— ëŒ€í•œ ë””ë²„ê·¸ ì •ë³´
                tool_results_turn.append(
                    {
                        "index": i,
                        "func_name": func_name,
                        "mcp_name": mcp_name,
                        "args": args,
                        "payload": payload,
                        "result": tool_result,
                    }
                )

                # tool ê²°ê³¼ë¥¼ role=toolë¡œ LLMì— í”¼ë“œë°±
                self.messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": f"tc_{i}",
                        "name": func_name,
                        "content": json.dumps(tool_result, ensure_ascii=False),
                    }
                )
                self.logger.info(f"[TOOLCALL {i}] appended tool message")

            # ì´ë²ˆ í„´ì˜ tool ê²°ê³¼ ì±„ìš°ê³ , í„´ ë””ë²„ê·¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            turn_info["tool_results"] = tool_results_turn
            turns_debug.append(turn_info)

            # âœ… ëª¨ë“  ë„êµ¬ í˜¸ì¶œì´ ì„±ê³µí•œ ê²½ìš° â†’ ë°”ë¡œ í•œ ë²ˆ ë” ë¶ˆëŸ¬ì„œ ìì—°ì–´ ìš”ì•½ í›„ ì¢…ë£Œ
            if all_ok:
                self.logger.info(
                    "[BRIDGE] All tool calls OK; requesting final natural-language answer from LLM."
                )
                t2 = time.perf_counter()
                resp2 = self.llm.chat.completions.create(
                    model=model,
                    messages=self.messages,      # ì—¬ê¸°ì—ëŠ” ì´ë¯¸ role=tool ë©”ì‹œì§€ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŒ
                    tool_choice="none",
                    temperature=self.temp,
                    max_tokens=self.max_tokens,
                )
                dt2 = time.perf_counter() - t2

                msg2 = resp2.choices[0].message
                self.logger.info(
                    f"[LLM FINAL] response id: {getattr(resp2, 'id', '<no-id>')} ({dt2:.2f}s), "
                    f"finish_reason={resp2.choices[0].finish_reason}"
                )
                self.logger.debug(f"[LLM FINAL] assistant content:\n{msg2.content}")

                final_text = (msg2.content or "").strip()

                if debug:
                    return {
                        "final_answer": final_text,
                        "turns": turns_debug,
                    }
                return final_text

            # âŒ ë„êµ¬ ì¤‘ ì¼ë¶€ ì‹¤íŒ¨ â†’ ë‹¤ìŒ í„´ì—ì„œ ë‹¤ì‹œ ê³„íš ì§œë„ë¡ í•œ ë²ˆ ë” LLM í˜¸ì¶œ
            self.logger.info(
                "[BRIDGE] Some tool calls failed; asking LLM for another plan."
            )
            continue

        # ì—¬ê¸°ê¹Œì§€ ì™”ë‹¤ëŠ” ê²ƒì€ self.max_turns ë²ˆ ëŒ ë•Œê¹Œì§€
        # 'ë„êµ¬ í˜¸ì¶œ ì—†ëŠ” ìµœì¢… ë‹µë³€'ì„ ëª» ë°›ì•˜ë‹¤ëŠ” ëœ»
        self.logger.warning(
            "[CLIENT] Max turns reached without final non-tool answer; returning last assistant_text."
        )
        if debug:
            return {
                "final_answer": assistant_text,
                "turns": turns_debug,
            }
        return assistant_text
